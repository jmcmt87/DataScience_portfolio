{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import cess_esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Hola, buenas tardes, quisiera un bocadillo de jamon y queso. Necesitamos 20 tartas de vainilla, ahora, es que tenemos cumpleaños hoy. Hola, si, quisieramos 8 shawarmas de pollo para esta noche. Hoy me pasare por el restaurante, tenedme listo los 2 bocadillos de tortilla que pido siempre, gracias. Hola, quisiera pedir 5 perritos calientes con mostaza para esta noche. Seria posible encargar 20 tartas de chocolate hoy?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')], [('Una', 'di0fs0'), ('portavoz', 'nccs000'), ('de', 'sps00'), ('EDF', 'np00000'), ('explicó', 'vmis3s0'), ('a', 'sps00'), ('EFE', 'np00000'), ('que', 'cs'), ('el', 'da0ms0'), ('proyecto', 'ncms000'), ('para', 'sps00'), ('la', 'da0fs0'), ('construcción', 'ncfs000'), ('de', 'sps00'), ('Altamira_2', 'np00000'), (',', 'Fc'), ('al', 'spcms'), ('norte', 'ncms000'), ('de', 'sps00'), ('Tampico', 'np00000'), (',', 'Fc'), ('prevé', 'vmm02s0'), ('la', 'da0fs0'), ('utilización', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('natural', 'aq0cs0'), ('como', 'cs'), ('combustible', 'ncms000'), ('principal', 'aq0cs0'), ('en', 'sps00'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('ciclo', 'ncms000'), ('combinado', 'aq0msp'), ('que', 'pr0cn000'), ('debe', 'vmip3s0'), ('empezar', 'vmn0000'), ('a', 'sps00'), ('funcionar', 'vmn0000'), ('en', 'sps00'), ('mayo_del_2002', 'W'), ('.', 'Fp')], ...]\n"
     ]
    }
   ],
   "source": [
    "sents = cess_esp.tagged_sents()\n",
    "print (sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5427\n",
      "603\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "test = []\n",
    "for i in range(len(sents)):\n",
    "    if i % 10:\n",
    "        training.append(sents[i])\n",
    "    else:\n",
    "        test.append(sents[i])\n",
    "\n",
    "print(len(training))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acierto con HMMs: 89.88905831011094\n"
     ]
    }
   ],
   "source": [
    "from nltk import UnigramTagger, BigramTagger, TrigramTagger\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "\n",
    "hmm_tagger = HiddenMarkovModelTagger.train(training)\n",
    "\n",
    "print ('Acierto con HMMs:', hmm_tagger.evaluate(test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = 'Hola, quiero un perrito caliente y ya está'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola, buenas tardes, quisiera un bocadillo de jamon y queso. Necesitamos 20 tartas de vainilla, ahora, es que tenemos cumpleaños hoy. Hola, si, quisieramos 8 shawarmas de pollo para esta noche. Hoy me pasare por el restaurante, tenedme listo los 2 bocadillos de tortilla que pido siempre, gracias. Hola, quisiera pedir 5 perritos calientes con mostaza para esta noche. Seria posible encargar 20 tartas de chocolate hoy? Hola, quiero un perrito caliente y ya está'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document3 = document + ' ' + document1\n",
    "document3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Tree\n",
    "\n",
    "def conversion(tree):\n",
    "    dlist = [] # Lista que incluira los diccionarios\n",
    "    d = dict() # Diccionarios creados con las categorias creadas con nuestra gramatica chunk\n",
    "    for item in tree:\n",
    "        if isinstance(item, Tree): # Si tenemos un arbol creamos un diccionario que contenga las hojas\n",
    "            d[item.label()] = ' '.join([l[0] for l in item.leaves()])\n",
    "        else:\n",
    "            dlist.append(d) if len(d)>0 else None # Cuando acabamos devolvemos el diccionario\n",
    "            d = dict()\n",
    "    dlist.append(d) if len(d)>0 else None # anadimos los diccionarios en la lista hasta el finl\n",
    "    return dlist\n",
    "\n",
    "def process_order(document):\n",
    "    '''Esta funcion comprueba de si se trata de un solo pedido o de varios, despues procesa la informacion\n",
    "    para sacar cantidades, comidas e ingredientes'''\n",
    "    \n",
    "    # Esta comprehension list nos dira si se trata de multiples pedidos o si se trata de uno solo\n",
    "    sentences = [sent for sent in nltk.sent_tokenize(document)]\n",
    "    \n",
    "    # Gramatica del chunk que nos permitira separar la informacion en cantidades, comidas e ingredientes\n",
    "    chunk_grammar = chunk_grammar = \"\"\"Comida: {<Z|di.*>+<nc.*>+<aq.*>?}\n",
    "                    }<Z|di.*>+{\n",
    "                    Cantidad: {<Z|di.*>+}\n",
    "                    Ingredientes: {<sp.*><n.*><cc>?<n.*>?}\n",
    "                    }<sp.*>+<cc>?{\"\"\" \n",
    "    \n",
    "    # Creamos el parser usando nuestra gramatica chunk\n",
    "    regex_parser = nltk.RegexpParser(chunk_grammar)\n",
    "    \n",
    "    if len(sentences) > 1: # Se trata de un pedido multiple\n",
    "        \n",
    "        # Parseamos nuestro texto usando nuestro regex_parser\n",
    "        parsed_text = [regex_parser.parse(hmm_tagger.tag(nltk.word_tokenize(sent))) for sent in nltk.sent_tokenize(document)]\n",
    "\n",
    "    \n",
    "        # Este bucle for introduce los diccionarios en una lista\n",
    "        info = []\n",
    "        for tree in parsed_text:\n",
    "            info.append(conversion(tree)) \n",
    "        \n",
    "        # Este bucle for une los diccionario de comida y cantidad con el de ingredientes, y desecha el duplicado ingredientes\n",
    "        for i in range(len(info)):\n",
    "        # Los ingredientes aparecen en una diccionario diferente por lo que este 'if' maneja la excepcion de que no haya ingredientes\n",
    "            if len(info[i]) > 1:\n",
    "                info[i][0].update(info[i][1])\n",
    "                info[i].pop()\n",
    "        \n",
    "        # Este bucle for desempaqueta los diccionarios de dentro de las listas.    \n",
    "        list_dictionaries = []\n",
    "        for i in range(len(info)):\n",
    "            list_dictionaries.append(info[i][0])\n",
    "        return list_dictionaries\n",
    "    \n",
    "    else: # Se trata de un solo pedido\n",
    "        parsed_text = [regex_parser.parse(hmm_tagger.tag(nltk.word_tokenize(document)))]\n",
    "        info = []\n",
    "        for tree in parsed_text:\n",
    "            info.append(conversion(tree)) \n",
    "        if len(info[0]) > 1:\n",
    "            info[0][0].update(info[0][1])\n",
    "            info[0].pop()\n",
    "        return info[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cantidad': 'un', 'Comida': 'perrito caliente'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_order(document1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Cantidad': 'un', 'Comida': 'bocadillo', 'Ingredientes': 'jamon y queso'},\n",
       " {'Cantidad': '20', 'Comida': 'tartas', 'Ingredientes': 'vainilla'},\n",
       " {'Cantidad': '8', 'Comida': 'shawarmas', 'Ingredientes': 'pollo'},\n",
       " {'Cantidad': '2', 'Comida': 'bocadillos', 'Ingredientes': 'tortilla'},\n",
       " {'Cantidad': '5', 'Comida': 'perritos calientes', 'Ingredientes': 'mostaza'},\n",
       " {'Cantidad': '20', 'Comida': 'tartas', 'Ingredientes': 'chocolate'},\n",
       " {'Cantidad': 'un', 'Comida': 'perrito caliente'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_order(document3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_iob_tags(document):\n",
    "    '''Esta funcion comprueba de si se trata de un solo pedido o de varios, despues procesa la informacion\n",
    "    para sacar cantidades, comidas e ingredientes'''\n",
    "    \n",
    "    # Esta comprehension list nos dira si se trata de multiples pedidos o si se trata de uno solo\n",
    "    sentences = [sent for sent in nltk.sent_tokenize(document)]\n",
    "    \n",
    "    # Gramatica del chunk que nos permitira separar la informacion en cantidades, comidas e ingredientes\n",
    "    chunk_grammar = chunk_grammar = \"\"\"Comida: {<Z|di.*>+<nc.*>+<aq.*>?}\n",
    "                    }<Z|di.*>+{\n",
    "                    Cantidad: {<Z|di.*>+}\n",
    "                    Ingredientes: {<sp.*><n.*><cc>?<n.*>?}\n",
    "                    }<sp.*>+<cc>?{\"\"\" \n",
    "    \n",
    "    # Creamos el parser usando nuestra gramatica chunk\n",
    "    regex_parser = nltk.RegexpParser(chunk_grammar)\n",
    "    \n",
    "    if len(sentences) > 1: # Se trata de un pedido multiple\n",
    "        \n",
    "        # Parseamos nuestro texto usando nuestro regex_parser\n",
    "        parsed_text = [regex_parser.parse(hmm_tagger.tag(nltk.word_tokenize(sent))) for sent in nltk.sent_tokenize(document)]\n",
    "\n",
    "    \n",
    "        # Este bucle for introduce los diccionarios en una lista\n",
    "        for parsed_sent in parsed_text:\n",
    "            list_of_lists = []\n",
    "            for i in range(len(parsed_text)):\n",
    "                # Filtramos el arbol padre S\n",
    "                for j in parsed_text[i].subtrees(filter = lambda x: x.label() == 'S'):\n",
    "                    # Obtenemos las etiquetas\n",
    "                    iob_tags = nltk.tree2conlltags(j)\n",
    "    \n",
    "                    words_tagged = []\n",
    "    \n",
    "                    for iob in iob_tags: \n",
    "                        words_tagged.append(iob)\n",
    "                list_of_lists.append(words_tagged)\n",
    "\n",
    "        return list_of_lists\n",
    "    \n",
    "    else: # Se trata de un solo pedido\n",
    "        parsed_text1 = [regex_parser.parse(hmm_tagger.tag(nltk.word_tokenize(sent))) for sent in nltk.sent_tokenize(document)]\n",
    "        list_of_lists1 = []\n",
    "        for i in parsed_text1[0].subtrees(filter = lambda x: x.label() == 'S'):\n",
    "            iob_tags = nltk.tree2conlltags(i)\n",
    "    \n",
    "            words_tagged = []\n",
    "    \n",
    "            for iob in iob_tags: \n",
    "                words_tagged.append(iob)\n",
    "                \n",
    "            list_of_lists1.append(words_tagged)\n",
    "            \n",
    "            return(list_of_lists1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Hola', 'np0000p', 'O'),\n",
       "  (',', 'Fc', 'O'),\n",
       "  ('quiero', 'vmis3s0', 'O'),\n",
       "  ('un', 'di0ms0', 'B-Cantidad'),\n",
       "  ('perrito', 'ncms000', 'B-Comida'),\n",
       "  ('caliente', 'aq0cs0', 'I-Comida'),\n",
       "  ('y', 'cc', 'O'),\n",
       "  ('ya', 'rg', 'O'),\n",
       "  ('está', 'vmip3s0', 'O')]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_iob_tags(document1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Hola', 'np0000p', 'O'),\n",
       "  (',', 'Fc', 'O'),\n",
       "  ('buenas', 'aq0fp0', 'O'),\n",
       "  ('tardes', 'ncfp000', 'O'),\n",
       "  (',', 'Fc', 'O'),\n",
       "  ('quisiera', 'vmis3s0', 'O'),\n",
       "  ('un', 'di0ms0', 'B-Cantidad'),\n",
       "  ('bocadillo', 'ncms000', 'B-Comida'),\n",
       "  ('de', 'sps00', 'O'),\n",
       "  ('jamon', 'np0000l', 'B-Ingredientes'),\n",
       "  ('y', 'cc', 'I-Ingredientes'),\n",
       "  ('queso', 'np0000p', 'I-Ingredientes'),\n",
       "  ('.', 'Fp', 'O')],\n",
       " [('Necesitamos', 'da0mp0', 'O'),\n",
       "  ('20', 'Z', 'B-Cantidad'),\n",
       "  ('tartas', 'ncmp000', 'B-Comida'),\n",
       "  ('de', 'sps00', 'O'),\n",
       "  ('vainilla', 'np0000l', 'B-Ingredientes'),\n",
       "  (',', 'Fc', 'O'),\n",
       "  ('ahora', 'rg', 'O'),\n",
       "  (',', 'Fc', 'O'),\n",
       "  ('es', 'vsip3s0', 'O'),\n",
       "  ('que', 'cs', 'O'),\n",
       "  ('tenemos', 'sn.e-SUJ', 'O'),\n",
       "  ('cumpleaños', 'vmis3s0', 'O'),\n",
       "  ('hoy', 'rg', 'O'),\n",
       "  ('.', 'Fp', 'O')],\n",
       " [('Hola', 'np0000p', 'O'),\n",
       "  (',', 'Fc', 'O'),\n",
       "  ('si', 'cs', 'O'),\n",
       "  (',', 'Fc', 'O'),\n",
       "  ('quisieramos', 'sps00', 'O'),\n",
       "  ('8', 'Z', 'B-Cantidad'),\n",
       "  ('shawarmas', 'ncmp000', 'B-Comida'),\n",
       "  ('de', 'sps00', 'O'),\n",
       "  ('pollo', 'ncms000', 'B-Ingredientes'),\n",
       "  ('para', 'sps00', 'O'),\n",
       "  ('esta', 'dd0fs0', 'O'),\n",
       "  ('noche', 'ncfs000', 'O'),\n",
       "  ('.', 'Fp', 'O')],\n",
       " [('Hoy', 'rg', 'O'),\n",
       "  ('me', 'pp1cs000', 'O'),\n",
       "  ('pasare', 'vmip3s0', 'O'),\n",
       "  ('por', 'sps00', 'O'),\n",
       "  ('el', 'da0ms0', 'O'),\n",
       "  ('restaurante', 'ncms000', 'O'),\n",
       "  (',', 'Fc', 'O'),\n",
       "  ('tenedme', 'vmis3s0', 'O'),\n",
       "  ('listo', 'cs', 'O'),\n",
       "  ('los', 'da0mp0', 'O'),\n",
       "  ('2', 'Z', 'B-Cantidad'),\n",
       "  ('bocadillos', 'ncmp000', 'B-Comida'),\n",
       "  ('de', 'sps00', 'O'),\n",
       "  ('tortilla', 'ncfs000', 'B-Ingredientes'),\n",
       "  ('que', 'pr0cn000', 'O'),\n",
       "  ('pido', 'vmip3s0', 'O'),\n",
       "  ('siempre', 'rg', 'O'),\n",
       "  (',', 'Fc', 'O'),\n",
       "  ('gracias', 'ncfp000', 'O'),\n",
       "  ('.', 'Fp', 'O')],\n",
       " [('Hola', 'np0000p', 'O'),\n",
       "  (',', 'Fc', 'O'),\n",
       "  ('quisiera', 'np0000p', 'O'),\n",
       "  ('pedir', 'Fpa', 'O'),\n",
       "  ('5', 'Z', 'B-Cantidad'),\n",
       "  ('perritos', 'ncmp000', 'B-Comida'),\n",
       "  ('calientes', 'aq0mpp', 'I-Comida'),\n",
       "  ('con', 'sps00', 'O'),\n",
       "  ('mostaza', 'ncfs000', 'B-Ingredientes'),\n",
       "  ('para', 'sps00', 'O'),\n",
       "  ('esta', 'dd0fs0', 'O'),\n",
       "  ('noche', 'ncfs000', 'O'),\n",
       "  ('.', 'Fp', 'O')],\n",
       " [('Seria', 'rg', 'O'),\n",
       "  ('posible', 'aq0cs0', 'O'),\n",
       "  ('encargar', 'Fp', 'O'),\n",
       "  ('20', 'Z', 'B-Cantidad'),\n",
       "  ('tartas', 'ncmp000', 'B-Comida'),\n",
       "  ('de', 'sps00', 'O'),\n",
       "  ('chocolate', 'ncms000', 'B-Ingredientes'),\n",
       "  ('hoy', 'rg', 'O'),\n",
       "  ('?', 'Fit', 'O')],\n",
       " [('Hola', 'np0000p', 'O'),\n",
       "  (',', 'Fc', 'O'),\n",
       "  ('quiero', 'vmis3s0', 'O'),\n",
       "  ('un', 'di0ms0', 'B-Cantidad'),\n",
       "  ('perrito', 'ncms000', 'B-Comida'),\n",
       "  ('caliente', 'aq0cs0', 'I-Comida'),\n",
       "  ('y', 'cc', 'O'),\n",
       "  ('ya', 'rg', 'O'),\n",
       "  ('está', 'vmip3s0', 'O')]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_iob_tags(document3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
